The authors introduced **Table Bank Dataset**, an innovative dataset for table detection and recognition, constructed using a novel weak supervision method derived from ***word*** and ***latex*** documents sourced from the internet. TableBank boasts an extensive collection of 417,000 high-quality labeled tables. Leveraging this dataset, the authors developed several robust baseline models employing state-of-the-art deep neural networks. Furthermore, the authors have made TableBank openly accessible to the public, aiming to catalyze advancements in deep learning methodologies for table detection and recognition tasks.

## Motivation

Table detection and recognition plays a crucial role in numerous document analysis applications, as tables often convey vital information in a structured manner. However, this task is inherently challenging due to the diverse layouts and formats of tables. Traditional techniques for table analysis have relied on document layout analysis, but these methods often struggle to generalize across different document types due to their reliance on handcrafted features, which lack robustness to layout variations. In recent years, the rapid advancements in deep learning within the field of computer vision have revolutionized data-driven image-based approaches to table analysis. Image-based table analysis offers the advantage of being robust to various document types, whether they are scanned images of pages or native digital formats. This approach can be applied to a wide range of document formats, including PDFs, HTML, PowerPoint, and their scanned versions, making it versatile in extracting tables. Despite some document types containing structured tabular data, there remains a need for a generalized approach to detect tables across different document formats. Consequently, large-scale end-to-end deep learning models have emerged as a viable solution, enabling improved performance in table detection and recognition tasks.

Current deep learning models for table analysis typically involve fine-tuning pre-trained object detection models using thousands of manually labeled training instances. However, scaling these models for real-world applications remains a challenge. Consequently, the only viable approach to developing open-domain table analysis models using deep learning is by increasing the size of the training data. Deep learning models are notably more complex than traditional models, often containing hundreds of millions of free parameters and requiring substantial amounts of labeled training data. Unfortunately, hand-labeling such extensive training sets is both costly and inflexible, presenting a significant bottleneck in deploying deep learning models effectively. Popular image classification and object detection datasets like [ImageNet](https://www.image-net.org/) and [COCO](https://cocodataset.org/#home) are typically built through crowdsourcing efforts, but this process is expensive and time-consuming, often taking months or even years to compile large benchmark datasets. Fortunately, there is a wealth of digital documents available on the internet, including ***word*** and ***latex*** source files. Leveraging weak supervision methods to label tables within these online documents could prove instrumental in addressing the labeling challenges associated with training data for deep learning models.

In response to the pressing need for a standardized open-domain table benchmark dataset, the authors have introduced a pioneering weak supervision approach to automatically generate Table Bank dataset. This dataset is significantly larger in scale compared to existing human-labeled datasets for table analysis. Unlike traditional weakly supervised training sets, our approach not only yields vast amounts of data but also ensures high quality. In today's digital landscape, there is an abundance of electronic documents available on the internet, including Microsoft Word (.docx) and LaTeX (.tex) files. These documents inherently contain markup tags for tables within their source code. Leveraging this, the authors can manipulate the source code by integrating bounding boxes using the markup language embedded within each document. In Word documents, they can modify the internal Office XML code to identify the boundaries of each table. Similarly, in LaTeX documents, they can adjust the tex code to recognize the bounding boxes of tables. This process enables the creation of high-quality labeled data across various domains, such as business documents, official filings, research papers, and more. Such a resource proves immensely valuable for large-scale table analysis tasks.

## Document acquisition

The Table Bank dataset totally consists of 417,234 high quality labeled tables as well as their original documents in a variety of domains. To verify the effectiveness of TableBank, they build several strong baselines using state of theart models with end-to-end deep neural networks. The authors create the TableBank dataset using two different file types: Word documents and Latex documents. Both file types contain mark-up tags for tables in their source code by nature. The authors scrape Word documents from the internet, all of which are in '.docx' format. This format allows them to modify the internal Office XML code, enabling the addition of bounding boxes. Without filtering the document language, these Word documents encompass a range of languages including English, Chinese, Japanese, Arabic, and others. This linguistic diversity enhances the dataset's versatility and resilience for real-world applications. In contrast, Latex documents require additional resources to compile into PDF files. Consequently, the authors cannot simply scrape the '.tex' files from the internet. Instead, they utilize documents sourced from the extensive pre-print database [arXiv.org](arXiv.org), along with their corresponding source code. The authors acquire Latex source code spanning from 2014 to 2018 via bulk data access on arXiv. Notably, the predominant language of these Latex documents is English.

## Table detection

The authors employ an intuitive approach to manipulate the source code by integrating bounding boxes using the markup language embedded within each document. Specifically, for Word documents, bounding boxes are added to tables by editing the internal Office XML code within each document. 

<img src="https://github.com/dataset-ninja/table-bank/assets/120389559/ce5afe44-71bb-456b-842a-a7326179a8a2" alt="image" width="600">

<span style="font-size: smaller; font-style: italic;">Data processing pipeline.</span>

In essence, each '.docx' file is a compressed archive file, containing a 'document.xml' file within the decompressed folder. Within this XML file, code snippets enclosed between the tags '<w:tbl>' and '</w:tbl>' typically represent tables in the Word document. The authors modify these code snippets within the XML file, altering the table borders to a distinct color compared to other parts of the document. Ultimately, they extract PDF pages from Word documents that contain at least one table on each page.

<img src="https://github.com/dataset-ninja/table-bank/assets/120389559/0192d2fb-e652-447b-874b-65fd39ce518c" alt="image" width="500">

<span style="font-size: smaller; font-style: italic;">The tables can be identified and labeled from “<w:tbl>” and “</w:tbl>” tags in the Office XML code</span>

For Latex documents, the authors use a special command in the Tex grammar ‘fcolorbox’ to add the bounding box to tables. Typically, the tables in Latex documents are usually in the format as follows:

<img src="https://github.com/dataset-ninja/table-bank/assets/120389559/136317af-e821-4195-8fa0-f26cf83c2094" alt="image" width="500">

They insert the ‘fcolorbox’ command to the table’s code as follows and re-compile the Latex documents. Meanwhile, the authors also define a special color so that the border is distinguishable. The overall process is similar to Word documents. Finally, they get PDF pages from Latex documents that contain at least one table on each page.

<img src="https://github.com/dataset-ninja/table-bank/assets/120389559/09d32e9c-a877-4af1-a00b-eaeb528c328b" alt="image" width="800">

To establish ground truth labels, the authors extract annotations from the generated PDF documents. For each page in the annotated PDFs, they overlay bounding boxes onto the tables in the original page using a white color, ensuring alignment between the two pages. Subsequently, they conduct a pixel-level comparison of the two pages to identify any discrepancies, enabling them to determine the upper left point of the tables, as well as their width and height parameters. Through this method, a total of 417,234 labeled tables are generated from the crawled documents. To assess the accuracy of the generated dataset, the authors randomly sample 1,000 examples and manually inspect the bounding boxes of the tables. They observe that only 5 examples are incorrectly labeled, indicating the high quality of the dataset.

## Table structure recognition

The objective of table structure recognition is to discern the layout of rows and columns within tables, particularly in non-digital document formats like scanned images. Existing models for table structure recognition typically identify both the layout information and the textual content of the cells. In this study, the authors delineate the task as follows: given a table in image format, generate an HTML tag sequence that encapsulates the arrangement of rows and columns, along with the classification of table cells. This approach allows for the automatic creation of a structure recognition dataset sourced from the source code of Word and LaTeX documents. For Word documents, the authors directly convert the original XML information from the documents into an HTML tag sequence. For LaTeX documents, they initially utilize the [LaTeXML toolkit](https://math.nist.gov/~BMiller/LaTeXML/) to generate XML from LaTeX, which is then transformed into HTML. After filtering out any noise, a total of 145,463 training instances are created from Word and LaTeX documents.
